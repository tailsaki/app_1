{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-06T06:24:46.350919Z","iopub.execute_input":"2022-10-06T06:24:46.351548Z","iopub.status.idle":"2022-10-06T06:24:46.379537Z","shell.execute_reply.started":"2022-10-06T06:24:46.351458Z","shell.execute_reply":"2022-10-06T06:24:46.378752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#float64のカラム3つを削除し、ランダムフォレストオブジェトで重要度の低い特徴量を削減。\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n#学習データ・テストデータの読み込み\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n#学習データを特徴量と目的変数に分ける\ntrain_x = train.drop(['Id','SalePrice','LotFrontage','MasVnrArea','GarageYrBlt'], axis=1)\ntrain_y = train['SalePrice']\n#テストデータ\ntest_x = test.drop(['Id','LotFrontage','MasVnrArea','GarageYrBlt'], axis=1)\ntest_y_answer = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv').drop(['Id'], axis=1)\n\n#それぞれのカテゴリ変数にlabel encodingを適用する\nx = pd.concat([train_x, test_x], axis=0)\ncat_cols = x.dtypes[train_x.dtypes=='object'].index.tolist()\nfor c in cat_cols:\n#学習データに基づいてどう変換するかを定める\n    le = LabelEncoder()\n    le.fit(x[c].fillna('NaN'))\n    \n    #学習データ・テストデータを変換する\n    train_x[c] = le.transform(train_x[c].fillna('NaN'))\n    test_x[c] = le.transform(test_x[c].fillna('NaN'))\n\n#ランダムフォレストオブジェトの生成(決定木の個数=500)\nforest = RandomForestClassifier(n_estimators=500, random_state=1)\n#モデルを適合\nforest.fit(train_x, train_y)\n#特徴量の重要度を抽出\nimportances = forest.feature_importances_\n#重要度の降順で特徴量のインデックスを抽出\nindices = np.argsort(importances)[::-1]\n#重要度の降順で特徴量の名称、重要度を表示\nfor f in range(train_x.shape[1]):\n    print(\"%2d) %-*s %f\" %\n         (f + 1, 30, train_x.columns[indices[f]], importances[indices[f]])\n         )","metadata":{"execution":{"iopub.status.busy":"2022-09-29T07:51:02.038184Z","iopub.execute_input":"2022-09-29T07:51:02.039076Z","iopub.status.idle":"2022-09-29T07:51:26.919155Z","shell.execute_reply.started":"2022-09-29T07:51:02.039027Z","shell.execute_reply":"2022-09-29T07:51:26.917720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bestなモデルに編集していく\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_x_2 = train_x[train_x.columns[indices[np.arange(27)]]]\ntest_x_2 = test_x[test_x.columns[indices[np.arange(27)]]]\n#バリデーション\ntr_x, va_x, tr_y, va_y = train_test_split(train_x_2, train_y,\n                                         test_size=0.25, random_state=71, shuffle=True)\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\ndtrain = xgb.DMatrix(tr_x, label=tr_y)\ndvalid = xgb.DMatrix(va_x, label=va_y)\ndtest = xgb.DMatrix(test_x_2)\n\n#ハイパーパラメータの設定\nparams = {'objective': 'reg:squarederror', 'random_state': 71}\nnum_round=50\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\nmodel = xgb.train(params, dtrain, num_round, early_stopping_rounds=20, evals=watchlist)\n\nva_pred = model.predict(dvalid)\nprint('バリデーションデータ予測値:')\nprint(va_pred)\n#平均平方二乗誤差でスコアを算出\nscore = np.sqrt(mean_squared_error(va_y, va_pred))\nprint('平均平方二乗誤差')\nprint(score)\n\n#予測(二値の予測値)\ntest_pred = model.predict(dtest)\nprint('test予測値:')\nprint(test_pred)\n#平均平方二乗誤差でスコアを算出\nscore_test = np.sqrt(mean_squared_error(test_y_answer, test_pred))\nprint('test平均平方二乗誤差')\nprint(score_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-29T07:52:08.846896Z","iopub.execute_input":"2022-09-29T07:52:08.847392Z","iopub.status.idle":"2022-09-29T07:52:09.471317Z","shell.execute_reply.started":"2022-09-29T07:52:08.847356Z","shell.execute_reply":"2022-09-29T07:52:09.470235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = {\n    'Id'       : test['Id'],\n    'Saleprice': test_pred\n}\ndf1 = pd.DataFrame(data_test)\nprint(df1)\n\ndf1.to_csv(\"test1.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-09-29T08:30:03.613159Z","iopub.execute_input":"2022-09-29T08:30:03.613668Z","iopub.status.idle":"2022-09-29T08:30:03.631921Z","shell.execute_reply.started":"2022-09-29T08:30:03.613618Z","shell.execute_reply":"2022-09-29T08:30:03.630406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#全特徴量を重要順にどこまで使用するかを確認する\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nscore_list = []\nscore_va_list = []\nscore_test_list = []\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n#Salepriceと関連性の高い順に特徴量を1~30個使用\nfor p in range(1,80):\n    train_x_2 = train_x[train_x.columns[indices[np.arange(p)]]]\n    test_x_2 = test_x[test_x.columns[indices[np.arange(p)]]]\n#バリデーション\n    for tr_idx, va_idx in kf.split(train_x_2):\n        tr_x, va_x = train_x_2.iloc[tr_idx], train_x_2.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\n        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n        dvalid = xgb.DMatrix(va_x, label=va_y)\n        dtest = xgb.DMatrix(test_x_2)\n\n#ハイパーパラメータの設定\n        params = {\n            'booster':'gbtree',\n            'objective': 'reg:squarederror',\n            'eta':0.1,\n            'gamma':0.0,\n            'alpha':0.0,\n            'lambda':1.0,\n            'min_child_weight':1,\n            'max_depth':5,\n            'subsample':0.8,\n            'colsample_bytree':0.8,\n            'random_state': 71\n        }\n        num_round=100\n\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\n        watchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\n        model = xgb.train(params, dtrain, num_round)\n\n        va_pred = model.predict(dvalid)\n#平均平方二乗誤差でスコアを算出\n        score = np.sqrt(mean_squared_error(va_y, va_pred))\n        score_list.append(score)\n        continue\n        \n    score_va_list.append(np.mean(score_list))\n    \n#test予測(二値の予測値)\n    test_pred = model.predict(dtest)\n#平均平方二乗誤差でtestスコアを算出\n    score_test = np.sqrt(mean_squared_error(test_y_answer, test_pred))\n    score_test_list.append(score_test)\n    \nprint('平均平方二乗誤差スコア')    \nfor f in range(len(score_va_list)):\n    print(\"%2d) バリデーションのスコア：%-*s 追加した特徴量は %-*s|　testのスコア：%5s\" \n          %(f + 1, \n            20, score_va_list[f],\n            30, train_x.columns[indices[f]],\n            score_test_list[f]\n           )\n          )","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-29T09:09:08.919285Z","iopub.execute_input":"2022-09-29T09:09:08.919731Z","iopub.status.idle":"2022-09-29T09:11:31.031055Z","shell.execute_reply.started":"2022-09-29T09:09:08.919696Z","shell.execute_reply":"2022-09-29T09:11:31.029751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ここから読み込む\n#float64のカラムのNaNを0に置換し、ランダムフォレストオブジェトで重要の高い順に特徴量を表示。\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n#学習データ・テストデータの読み込み\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n#学習データを特徴量と目的変数に分ける\ntrain_x = train.drop(['Id','SalePrice'], axis=1)\ntrain_y = train['SalePrice']\n#テストデータ\ntest_x = test.drop(['Id'], axis=1)\ntest_y_answer = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv').drop(['Id'], axis=1)\n\n\n#それぞれのカテゴリ変数にlabel encodingを適用する\nx = pd.concat([train_x, test_x], axis=0)\ncat_cols = x.dtypes[train_x.dtypes=='object'].index.tolist()\n#floatの特徴量をintに変換する\ntrain_x[['LotFrontage','MasVnrArea','GarageYrBlt']] = train_x[['LotFrontage','MasVnrArea','GarageYrBlt']].fillna(0).astype(int)\ntest_x[['LotFrontage','MasVnrArea','GarageYrBlt']] =test_x[['LotFrontage','MasVnrArea','GarageYrBlt']].fillna(0).astype(int)\nfor c in cat_cols:\n#学習データに基づいてどう変換するかを定める\n    le = LabelEncoder()\n    le.fit(x[c].fillna('NaN'))\n    #学習データ・テストデータを変換する\n    train_x[c] = le.transform(train_x[c].fillna('NaN'))\n    test_x[c] = le.transform(test_x[c].fillna('NaN'))\n\n#ランダムフォレストオブジェトの生成(決定木の個数=500)\nforest = RandomForestClassifier(n_estimators=500, random_state=1)\n#モデルを適合\nforest.fit(train_x, train_y)\n#特徴量の重要度を抽出\nimportances = forest.feature_importances_\n#重要度の降順で特徴量のインデックスを抽出\nindices = np.argsort(importances)[::-1]\n#重要度の降順で特徴量の名称、重要度を表示\nfor f in range(train_x.shape[1]):\n    print(\"%2d) %-*s %f\" %\n         (f + 1, 30, train_x.columns[indices[f]], importances[indices[f]])\n         )","metadata":{"execution":{"iopub.status.busy":"2022-10-03T15:16:18.494648Z","iopub.execute_input":"2022-10-03T15:16:18.495816Z","iopub.status.idle":"2022-10-03T15:16:48.177969Z","shell.execute_reply.started":"2022-10-03T15:16:18.495769Z","shell.execute_reply":"2022-10-03T15:16:48.176697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#予測をcsvで保存して、試しに提出する\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n#ハイパーパラメータの設定\nscore_list = []\nparams = {\n        'booster':'gbtree',\n        'objective': 'reg:squarederror',\n        'eta':0.01,\n        'gamma':0.0,\n        'alpha':0.0,\n        'lambda':1.0,\n        'min_child_weight':2,\n        'max_depth':6,\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'colsample_bylevel':0.4,\n        'random_state': 71\n        }\nnum_round=1000\n\ntrain_x_2 = train_x\ntest_x_2 = test_x\n\n#バリデーション\nfor tr_idx, va_idx in kf.split(train_x_2):\n    tr_x, va_x = train_x_2.iloc[tr_idx], train_x_2.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\n    dtrain = xgb.DMatrix(tr_x, label=tr_y)\n    dvalid = xgb.DMatrix(va_x, label=va_y)\n\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\n    watchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\n    model = xgb.train(params, dtrain, num_round)\n\n    va_pred = model.predict(dvalid)\n#平均平方二乗誤差でスコアを算出\n    score = np.sqrt(mean_squared_error(va_y, va_pred))\n    score_list.append(score)\n    continue\n        \nprint(np.mean(score_list))\n    \n#test予測(二値の予測値)\ndtrain = xgb.DMatrix(train_x_2, label=train_y)\ndtest = xgb.DMatrix(test_x_2)\n\nmodel = xgb.train(params, dtrain, num_round)\ntest_pred = model.predict(dtest)\n\ndata2 = {\n    'Id'       : test['Id'],\n    'Saleprice': test_pred\n}\ndf2 = pd.DataFrame(data2)\nprint(df2)\n\ndf2.to_csv(\"test6.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T05:41:55.563452Z","iopub.execute_input":"2022-10-03T05:41:55.563863Z","iopub.status.idle":"2022-10-03T05:42:16.233917Z","shell.execute_reply.started":"2022-10-03T05:41:55.563807Z","shell.execute_reply":"2022-10-03T05:42:16.233000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#手動でパラメータチューニングする場合\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n#ハイパーパラメータの設定\nfor i in [0.0,0.1,0.2,0.3,0.4]:\n    score_list = []\n    params = {\n        'booster':'gbtree',\n        'objective': 'reg:squarederror',\n        'eta':0.01,\n        'gamma':i,\n        'alpha':0.0,\n        'lambda':1.0,\n        'min_child_weight':2,\n        'max_depth':6,\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'colsample_bylevel':0.4,\n        'random_state': 71\n        }\n    num_round=1000\n\n    train_x_2 = train_x\n    test_x_2 = test_x\n\n#バリデーション\n    for tr_idx, va_idx in kf.split(train_x_2):\n        tr_x, va_x = train_x_2.iloc[tr_idx], train_x_2.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\n        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n        dvalid = xgb.DMatrix(va_x, label=va_y)\n\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\n        watchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\n        model = xgb.train(params, dtrain, num_round)\n\n        va_pred = model.predict(dvalid)\n#平均平方二乗誤差でスコアを算出\n        score = np.sqrt(mean_squared_error(va_y, va_pred))\n        score_list.append(score)\n        continue\n        \n    print(\"i = %s\" %(i))    \n    print(np.mean(score_list))\n    \n#test予測(二値の予測値)\n    dtrain = xgb.DMatrix(train_x_2, label=train_y)\n    dtest = xgb.DMatrix(test_x_2)\n\n    model = xgb.train(params, dtrain, num_round)\n    test_pred = model.predict(dtest)\n\n    data2 = {\n        'Id'       : test['Id'],\n        'Saleprice': test_pred\n    }\n    df2 = pd.DataFrame(data2)\n    print(df2)\n\n    df2.to_csv(\"test%s.csv\" %(i), index = False)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T05:39:03.070644Z","iopub.execute_input":"2022-10-03T05:39:03.071086Z","iopub.status.idle":"2022-10-03T05:40:50.539863Z","shell.execute_reply.started":"2022-10-03T05:39:03.071048Z","shell.execute_reply":"2022-10-03T05:40:50.538936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#グリッドサーチでパラメータチューニング\nimport itertools\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n#ハイパーパラメータの設定\nparam_space = {\n    'max_depth':[2,3,4,5,6,7,8,9,10],\n    'min_child_weight':[1,2,3,4],\n    }\nparam_combinations = itertools.product(param_space['max_depth'],param_space['min_child_weight'])\nparameters = []\nscores = []\nscore_list = []\nnum_round=1000\ntrain_x_2 = train_x\ntest_x_2 = test_x\n\nfor  max_depth, min_child_weight in param_combinations:\n    params = {\n        'booster':'gbtree',\n        'objective': 'reg:squarederror',\n        'eta':0.01,\n        'gamma':0.0,\n        'alpha':0.0,\n        'lambda':1.0,\n        'max_depth':max_depth,\n        'min_child_weight':min_child_weight,\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'colsample_bylevel':0.4,\n        'random_state': 71\n        }\n\n#バリデーション\n    for tr_idx, va_idx in kf.split(train_x_2):\n        tr_x, va_x = train_x_2.iloc[tr_idx], train_x_2.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\n        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n        dvalid = xgb.DMatrix(va_x, label=va_y)\n\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\n        watchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\n        model = xgb.train(params, dtrain, num_round)\n\n        va_pred = model.predict(dvalid)\n#平均平方二乗誤差でスコアを算出\n        score = np.sqrt(mean_squared_error(va_y, va_pred))\n        score_list.append(score)\n        continue\n    print(f'max_depth: %s, min_chind_weight: %s'%(max_depth, min_child_weight))\n    print(np.mean(score_list))\n    \n    parameters.append((max_depth,min_child_weight))\n    scores.append(np.mean(score_list))\n\nbest_idx = np.argsort(scores)[0]\nbest_param = parameters[best_idx]\nprint(f'max_depth: {best_param[0]}, min_chind_weight: {best_param[1]}')","metadata":{"execution":{"iopub.status.busy":"2022-10-03T15:16:48.180169Z","iopub.execute_input":"2022-10-03T15:16:48.180535Z","iopub.status.idle":"2022-10-03T15:27:19.525083Z","shell.execute_reply.started":"2022-10-03T15:16:48.180503Z","shell.execute_reply":"2022-10-03T15:27:19.524082Z"},"trusted":true},"execution_count":null,"outputs":[]}]}