{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-25T14:55:16.586172Z","iopub.execute_input":"2022-12-25T14:55:16.586660Z","iopub.status.idle":"2022-12-25T14:55:16.600503Z","shell.execute_reply.started":"2022-12-25T14:55:16.586620Z","shell.execute_reply":"2022-12-25T14:55:16.598956Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/ames-housing-dataset/AmesHousing.csv\n/kaggle/input/housingdata/housingdata.csv\n/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#学習データ・テストデータの読み込み\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-25T14:55:17.289156Z","iopub.execute_input":"2022-12-25T14:55:17.289918Z","iopub.status.idle":"2022-12-25T14:55:17.363110Z","shell.execute_reply.started":"2022-12-25T14:55:17.289871Z","shell.execute_reply":"2022-12-25T14:55:17.361601Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n\n  YrSold  SaleType  SaleCondition  SalePrice  \n0   2008        WD         Normal     208500  \n1   2007        WD         Normal     181500  \n2   2008        WD         Normal     223500  \n3   2006        WD        Abnorml     140000  \n4   2008        WD         Normal     250000  \n\n[5 rows x 81 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 81 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#float64のカラム3つを削除し、ランダムフォレストオブジェトで重要度の低い特徴量を削減。\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n#学習データを特徴量と目的変数に分ける\ntrain_x = train.drop(['Id','SalePrice','LotFrontage','MasVnrArea','GarageYrBlt'], axis=1)\ntrain_y = train['SalePrice']\n#テストデータ\ntest_x = test.drop(['Id','LotFrontage','MasVnrArea','GarageYrBlt'], axis=1)\ntest_y_answer = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv').drop(['Id'], axis=1)\n\n#それぞれのカテゴリ変数にlabel encodingを適用する\nx = pd.concat([train_x, test_x], axis=0)\ncat_cols = x.dtypes[train_x.dtypes=='object'].index.tolist()\nfor c in cat_cols:\n#学習データに基づいてどう変換するかを定める\n    le = LabelEncoder()\n    le.fit(x[c].fillna('NaN'))\n    \n    #学習データ・テストデータを変換する\n    train_x[c] = le.transform(train_x[c].fillna('NaN'))\n    test_x[c] = le.transform(test_x[c].fillna('NaN'))\n\n#ランダムフォレストオブジェトの生成(決定木の個数=500)\nforest = RandomForestClassifier(n_estimators=500, random_state=1)\n#モデルを適合\nforest.fit(train_x, train_y)\n#特徴量の重要度を抽出\nimportances = forest.feature_importances_\n#重要度の降順で特徴量のインデックスを抽出\nindices = np.argsort(importances)[::-1]\n#重要度の降順で特徴量の名称、重要度を表示\nfor f in range(train_x.shape[1]):\n    print(\"%2d) %-*s %f\" %\n         (f + 1, 30, train_x.columns[indices[f]], importances[indices[f]])\n         )","metadata":{"execution":{"iopub.status.busy":"2022-12-25T14:55:18.893430Z","iopub.execute_input":"2022-12-25T14:55:18.893915Z","iopub.status.idle":"2022-12-25T14:55:43.260407Z","shell.execute_reply.started":"2022-12-25T14:55:18.893872Z","shell.execute_reply":"2022-12-25T14:55:43.259052Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":" 1) LotArea                        0.041329\n 2) GrLivArea                      0.040556\n 3) 1stFlrSF                       0.038600\n 4) GarageArea                     0.038137\n 5) BsmtUnfSF                      0.037665\n 6) TotalBsmtSF                    0.037616\n 7) YearBuilt                      0.033952\n 8) MoSold                         0.033581\n 9) YearRemodAdd                   0.032154\n10) BsmtFinSF1                     0.031500\n11) OpenPorchSF                    0.027963\n12) WoodDeckSF                     0.026037\n13) Neighborhood                   0.025435\n14) YrSold                         0.023691\n15) TotRmsAbvGrd                   0.022272\n16) 2ndFlrSF                       0.021182\n17) Exterior2nd                    0.019766\n18) Exterior1st                    0.019309\n19) OverallQual                    0.018859\n20) BsmtFinType1                   0.017564\n21) OverallCond                    0.016075\n22) FireplaceQu                    0.015439\n23) BedroomAbvGr                   0.014740\n24) MSSubClass                     0.014496\n25) BsmtExposure                   0.013681\n26) GarageFinish                   0.013253\n27) MasVnrType                     0.013081\n28) HeatingQC                      0.012450\n29) Fireplaces                     0.012387\n30) LotConfig                      0.012022\n31) HouseStyle                     0.011712\n32) LotShape                       0.011131\n33) GarageType                     0.010414\n34) EnclosedPorch                  0.010057\n35) Condition1                     0.009943\n36) BsmtFullBath                   0.009815\n37) BsmtQual                       0.009749\n38) KitchenQual                    0.009711\n39) Fence                          0.009705\n40) SaleCondition                  0.009666\n41) GarageCars                     0.009619\n42) Foundation                     0.009619\n43) BsmtFinSF2                     0.009092\n44) RoofStyle                      0.009048\n45) ScreenPorch                    0.008305\n46) MSZoning                       0.008122\n47) HalfBath                       0.007872\n48) SaleType                       0.007726\n49) BsmtFinType2                   0.007530\n50) ExterQual                      0.007020\n51) FullBath                       0.006992\n52) ExterCond                      0.006698\n53) BsmtCond                       0.006395\n54) LandContour                    0.006078\n55) BldgType                       0.005991\n56) Functional                     0.005310\n57) GarageQual                     0.004622\n58) Electrical                     0.004419\n59) Alley                          0.004281\n60) BsmtHalfBath                   0.004120\n61) GarageCond                     0.003969\n62) MiscVal                        0.003909\n63) PavedDrive                     0.003786\n64) LandSlope                      0.003612\n65) 3SsnPorch                      0.003226\n66) MiscFeature                    0.002752\n67) CentralAir                     0.002486\n68) LowQualFinSF                   0.002270\n69) Heating                        0.002041\n70) KitchenAbvGr                   0.001940\n71) RoofMatl                       0.001647\n72) Condition2                     0.001266\n73) Street                         0.000513\n74) PoolArea                       0.000500\n75) PoolQC                         0.000428\n76) Utilities                      0.000104\n","output_type":"stream"}]},{"cell_type":"code","source":"#bestなモデルに編集していく\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_x_2 = train_x[train_x.columns[indices[np.arange(27)]]]\ntest_x_2 = test_x[test_x.columns[indices[np.arange(27)]]]\n#バリデーション\ntr_x, va_x, tr_y, va_y = train_test_split(train_x_2, train_y,\n                                         test_size=0.25, random_state=71, shuffle=True)\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\ndtrain = xgb.DMatrix(tr_x, label=tr_y)\ndvalid = xgb.DMatrix(va_x, label=va_y)\ndtest = xgb.DMatrix(test_x_2)\n\n#ハイパーパラメータの設定\nparams = {'objective': 'reg:squarederror', 'random_state': 71}\nnum_round=50\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\nmodel = xgb.train(params, dtrain, num_round, early_stopping_rounds=20, evals=watchlist)\n\nva_pred = model.predict(dvalid)\nprint('バリデーションデータ予測値:')\nprint(va_pred)\n#平均平方二乗誤差でスコアを算出\nscore = np.sqrt(mean_squared_error(va_y, va_pred))\nprint('平均平方二乗誤差')\nprint(score)\n\n#予測(二値の予測値)\ntest_pred = model.predict(dtest)\nprint('test予測値:')\nprint(test_pred)\n#平均平方二乗誤差でスコアを算出\nscore_test = np.sqrt(mean_squared_error(test_y_answer, test_pred))\nprint('test平均平方二乗誤差')\nprint(score_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T14:55:43.263345Z","iopub.execute_input":"2022-12-25T14:55:43.263886Z","iopub.status.idle":"2022-12-25T14:55:44.034916Z","shell.execute_reply.started":"2022-12-25T14:55:43.263815Z","shell.execute_reply":"2022-12-25T14:55:44.033737Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[0]\ttrain-rmse:142182.65314\teval-rmse:141286.72626\n[1]\ttrain-rmse:102897.95056\teval-rmse:103162.64598\n[2]\ttrain-rmse:75096.88650\teval-rmse:76918.77664\n[3]\ttrain-rmse:55456.95963\teval-rmse:59127.01066\n[4]\ttrain-rmse:41728.29741\teval-rmse:48611.29635\n[5]\ttrain-rmse:31884.58681\teval-rmse:41286.15316\n[6]\ttrain-rmse:25028.34725\teval-rmse:37552.98345\n[7]\ttrain-rmse:20288.37973\teval-rmse:34775.90865\n[8]\ttrain-rmse:17060.81651\teval-rmse:33479.87466\n[9]\ttrain-rmse:14649.26467\teval-rmse:32414.31173\n[10]\ttrain-rmse:13124.88266\teval-rmse:31862.83466\n[11]\ttrain-rmse:11963.89744\teval-rmse:31723.28861\n[12]\ttrain-rmse:11221.42968\teval-rmse:31641.26040\n[13]\ttrain-rmse:10655.88906\teval-rmse:31283.00922\n[14]\ttrain-rmse:10135.98417\teval-rmse:31093.45536\n[15]\ttrain-rmse:9742.38619\teval-rmse:30943.37007\n[16]\ttrain-rmse:9445.13180\teval-rmse:31018.66678\n[17]\ttrain-rmse:9137.45411\teval-rmse:30950.84931\n[18]\ttrain-rmse:8924.08790\teval-rmse:30963.01436\n[19]\ttrain-rmse:8725.53075\teval-rmse:30906.36122\n[20]\ttrain-rmse:8462.74794\teval-rmse:30970.08792\n[21]\ttrain-rmse:7910.94874\teval-rmse:31051.70908\n[22]\ttrain-rmse:7691.76044\teval-rmse:31152.74269\n[23]\ttrain-rmse:7337.15730\teval-rmse:31086.46225\n[24]\ttrain-rmse:7036.89529\teval-rmse:31108.08941\n[25]\ttrain-rmse:6959.23558\teval-rmse:31125.26202\n[26]\ttrain-rmse:6756.83278\teval-rmse:31056.49699\n[27]\ttrain-rmse:6613.37291\teval-rmse:31060.43221\n[28]\ttrain-rmse:6429.90215\teval-rmse:31031.93703\n[29]\ttrain-rmse:6059.75460\teval-rmse:30973.21266\n[30]\ttrain-rmse:5877.56504\teval-rmse:31010.67589\n[31]\ttrain-rmse:5571.84425\teval-rmse:31032.07597\n[32]\ttrain-rmse:5425.95558\teval-rmse:31062.81839\n[33]\ttrain-rmse:5237.55639\teval-rmse:31057.19414\n[34]\ttrain-rmse:5024.31199\teval-rmse:31023.46856\n[35]\ttrain-rmse:4949.03305\teval-rmse:31009.32942\n[36]\ttrain-rmse:4866.63194\teval-rmse:31001.47741\n[37]\ttrain-rmse:4773.73691\teval-rmse:30991.52736\n[38]\ttrain-rmse:4665.01542\teval-rmse:31010.95657\nバリデーションデータ予測値:\n[227098.58   83063.91  167294.33   94532.79  181543.81  350646.28\n 169719.47  152116.2   216059.73  147723.47  103519.49  192024.95\n 338717.62  110387.02  138592.47  232711.06  131871.28  143732.\n  76998.4   124343.52  123187.15   59551.336  90902.15  188169.84\n 143575.28  100926.8   251710.62  143354.4   216055.95  104723.95\n 202621.33  126781.78  131847.22  133478.89  159421.9   101569.15\n 185260.14  281148.66  210976.    140905.47  195742.08  310094.88\n 140187.06  141374.03  237962.31  125804.23  170566.45  264140.06\n 145217.89  237884.64  223839.34  206858.2   106642.04   83413.71\n  92879.86  112074.18  275208.56  117501.09  187563.17  130072.19\n 115072.52  100751.445 163976.8   122277.82  187989.8   102106.55\n  86369.234 167029.34  222179.47  177523.92  297675.34  124548.53\n 207463.8   348422.28  146637.11  130682.484 320749.22  164372.\n 128122.79  175561.33  447394.66   91948.914 136714.42  182092.72\n 145028.77  210397.88  232026.03  124148.6   155780.61  237961.\n 283045.44  142009.77  172718.14  212767.94  150167.23  276427.8\n 123412.31  215752.72  101332.44  205739.08  191563.22  295859.38\n 109048.62  144128.14  192557.25  102043.59  161233.02  188135.36\n 185205.84  293620.38  396853.1   118674.79  155052.7   304493.25\n 107499.945 283438.7   171268.03  299166.78  105709.99  226588.9\n 139049.06  160057.58  195001.83  198449.3   126018.69  199269.66\n 153391.62  199889.9   259857.97  127445.04  132997.94  219835.3\n 315107.28  236361.16  119922.914 537703.1   120453.555 142051.98\n 129540.34  173215.67  114080.6   183392.62  198531.1   106518.94\n 319097.8   163535.55  170621.47  112307.72  103646.14  221271.88\n 190738.19  207430.55  134461.45  123627.51  421103.4   218446.67\n 154621.44  181705.8   158462.78  156988.27  151815.08  227433.52\n 164681.95  134538.84  163983.66   79040.33  170588.1   107166.664\n 150522.2   130538.055 343859.16  184327.67  232949.11  129446.984\n 132908.77  179150.88  185227.02   83861.77  209857.78  129863.75\n 197522.39  129219.24  156371.08  323236.16  162331.16  153607.55\n 153340.25  174956.7   140493.83  157773.56  113437.49  278962.6\n 131126.58  302112.34  137820.72   79760.51  141891.48  167082.64\n 165046.84  140585.45  258674.78  149076.27  119935.02  144633.36\n 225717.86   91052.24  132798.78   75821.17  261951.6   158994.78\n 160852.88  125452.51  203367.31  150558.97  338181.75  163855.3\n  80733.3   198667.81  302036.44  340913.78  163500.86  212515.58\n 236449.83  271499.4   292366.56   77059.195 134063.53  189887.36\n 207188.62  225158.05  203864.56  193127.8   117320.03  186456.02\n 213216.25  197943.73  228904.19  154966.58  106480.13  295542.22\n 188326.81  169905.17  124123.734 175561.6   277341.3   184758.2\n 191347.12  142611.03  135095.33  178650.95  140966.98  128761.164\n 105140.62  136086.45  207911.92  176091.4   155303.9   130722.18\n 131065.5   132106.12  193195.19  142744.9   140100.7   195458.1\n 157766.78  156656.08  145812.89  169820.62  178695.64  118004.414\n 201852.12  278788.22  302923.78  111068.27  100268.336 111815.88\n 178495.31  143056.67  182251.78  128843.67  159886.5   253542.52\n  77554.65  190031.45  128182.59  262031.83  367424.25  122633.09\n  81850.87  136744.1   137576.55  143191.86  136595.36  257051.28\n 123269.87  172623.16  203949.58  127849.34  228983.38  140321.84\n 140578.02  123148.43  171028.28  108897.92  163762.95  167562.67\n  90033.375 123585.73  194487.56  189306.03   88784.125 215707.72\n 137887.48  222991.48  179746.45  194754.95  206527.77  137217.5\n 271899.94  191703.62  291948.66  157404.62  174209.22  208921.27\n 131977.25  246230.73  196558.86  120483.66  150459.45  130487.086\n 122709.4   166478.89  121138.086 241554.23  153530.22  150556.23\n 134559.33  300508.94  322807.88  125429.42  160102.1   142402.95\n 126892.23  224759.3   357608.1   156268.22  250484.06  254324.36\n 127487.664 126149.1   123679.555 337230.97  267928.94  208333.34\n 275368.97  362141.5   113394.56  149608.7   142629.92  105127.15\n 129914.07  283601.8   366644.34  196651.11  149414.7  ]\n平均平方二乗誤差\n31033.07647817325\ntest予測値:\n[130901.734 159854.69  183813.56  ... 163176.05  113826.9   232833.83 ]\ntest平均平方二乗誤差\n73504.43514906483\n","output_type":"stream"}]},{"cell_type":"code","source":"#ここから読み込む\n#float64のカラムのNaNを0に置換し、ランダムフォレストオブジェトで重要の高い順に特徴量を表示。\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n#学習データ・テストデータの読み込み\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n#学習データを特徴量と目的変数に分ける\ntrain_x = train.drop(['Id','SalePrice'], axis=1)\ntrain_y = train['SalePrice']\n#テストデータ\ntest_x = test.drop(['Id'], axis=1)\ntest_y_answer = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv').drop(['Id'], axis=1)\n\n\n#それぞれのカテゴリ変数にlabel encodingを適用する\nx = pd.concat([train_x, test_x], axis=0)\ncat_cols = x.dtypes[train_x.dtypes=='object'].index.tolist()\n#floatの特徴量をintに変換する\ntrain_x[['LotFrontage','MasVnrArea','GarageYrBlt']] = train_x[['LotFrontage','MasVnrArea','GarageYrBlt']].fillna(0).astype(int)\ntest_x[['LotFrontage','MasVnrArea','GarageYrBlt']] =test_x[['LotFrontage','MasVnrArea','GarageYrBlt']].fillna(0).astype(int)\nfor c in cat_cols:\n#学習データに基づいてどう変換するかを定める\n    le = LabelEncoder()\n    le.fit(x[c].fillna('NaN'))\n    #学習データ・テストデータを変換する\n    train_x[c] = le.transform(train_x[c].fillna('NaN'))\n    test_x[c] = le.transform(test_x[c].fillna('NaN'))\n\n#ランダムフォレストオブジェトの生成(決定木の個数=500)\nforest = RandomForestClassifier(n_estimators=500, random_state=1)\n#モデルを適合\nforest.fit(train_x, train_y)\n#特徴量の重要度を抽出\nimportances = forest.feature_importances_\n#重要度の降順で特徴量のインデックスを抽出\nindices = np.argsort(importances)[::-1]\n#重要度の降順で特徴量の名称、重要度を表示\nfor f in range(train_x.shape[1]):\n    print(\"%2d) %-*s %f\" %\n         (f + 1, 30, train_x.columns[indices[f]], importances[indices[f]])\n         )","metadata":{"execution":{"iopub.status.busy":"2022-12-25T15:03:42.623206Z","iopub.execute_input":"2022-12-25T15:03:42.624908Z","iopub.status.idle":"2022-12-25T15:04:07.862995Z","shell.execute_reply.started":"2022-12-25T15:03:42.624853Z","shell.execute_reply":"2022-12-25T15:04:07.861415Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":" 1) LotArea                        0.038527\n 2) GrLivArea                      0.037582\n 3) 1stFlrSF                       0.035614\n 4) BsmtUnfSF                      0.035088\n 5) GarageArea                     0.034717\n 6) TotalBsmtSF                    0.034379\n 7) YearBuilt                      0.030571\n 8) LotFrontage                    0.030519\n 9) MoSold                         0.030191\n10) BsmtFinSF1                     0.029362\n11) GarageYrBlt                    0.029068\n12) YearRemodAdd                   0.028189\n13) OpenPorchSF                    0.025271\n14) WoodDeckSF                     0.023985\n15) Neighborhood                   0.023565\n16) YrSold                         0.021617\n17) MasVnrArea                     0.020946\n18) TotRmsAbvGrd                   0.020356\n19) 2ndFlrSF                       0.020337\n20) Exterior2nd                    0.018586\n21) OverallQual                    0.017521\n22) Exterior1st                    0.017423\n23) BsmtFinType1                   0.016229\n24) OverallCond                    0.014993\n25) FireplaceQu                    0.014211\n26) MSSubClass                     0.013696\n27) BedroomAbvGr                   0.013602\n28) GarageFinish                   0.012474\n29) BsmtExposure                   0.012391\n30) MasVnrType                     0.011289\n31) HeatingQC                      0.011249\n32) HouseStyle                     0.011112\n33) LotConfig                      0.010875\n34) Fireplaces                     0.010790\n35) LotShape                       0.009884\n36) EnclosedPorch                  0.009395\n37) KitchenQual                    0.009360\n38) GarageType                     0.009344\n39) Condition1                     0.009174\n40) BsmtFullBath                   0.009093\n41) BsmtQual                       0.008988\n42) GarageCars                     0.008891\n43) SaleCondition                  0.008830\n44) Foundation                     0.008757\n45) Fence                          0.008718\n46) BsmtFinSF2                     0.008604\n47) RoofStyle                      0.008518\n48) ScreenPorch                    0.007706\n49) HalfBath                       0.007453\n50) MSZoning                       0.007395\n51) BsmtFinType2                   0.007357\n52) SaleType                       0.007067\n53) ExterQual                      0.006577\n54) FullBath                       0.006404\n55) ExterCond                      0.006355\n56) BsmtCond                       0.005677\n57) LandContour                    0.005511\n58) BldgType                       0.005209\n59) Functional                     0.005025\n60) Electrical                     0.004148\n61) GarageQual                     0.004050\n62) Alley                          0.004009\n63) BsmtHalfBath                   0.003766\n64) GarageCond                     0.003691\n65) PavedDrive                     0.003476\n66) LandSlope                      0.003474\n67) MiscVal                        0.003442\n68) 3SsnPorch                      0.002950\n69) MiscFeature                    0.002631\n70) CentralAir                     0.002440\n71) LowQualFinSF                   0.002140\n72) Heating                        0.002088\n73) KitchenAbvGr                   0.001916\n74) RoofMatl                       0.001435\n75) Condition2                     0.001230\n76) PoolArea                       0.000494\n77) PoolQC                         0.000481\n78) Street                         0.000407\n79) Utilities                      0.000115\n","output_type":"stream"}]},{"cell_type":"code","source":"#予測をcsvで保存して提出する\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n#ハイパーパラメータの設定\nscore_list = []\nparams = {\n        'booster':'gbtree',\n        'objective': 'reg:squarederror',\n        'eta':0.01,\n        'gamma':0.0,\n        'alpha':0.0,\n        'lambda':1.0,\n        'min_child_weight':2,\n        'max_depth':6,\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'colsample_bylevel':0.4,\n        'random_state': 71\n        }\nnum_round=1000\n\ntrain_x_2 = train_x\ntest_x_2 = test_x\n\n#バリデーション\nfor tr_idx, va_idx in kf.split(train_x_2):\n    tr_x, va_x = train_x_2.iloc[tr_idx], train_x_2.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\n    dtrain = xgb.DMatrix(tr_x, label=tr_y)\n    dvalid = xgb.DMatrix(va_x, label=va_y)\n\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\n    watchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\n    model = xgb.train(params, dtrain, num_round)\n\n    va_pred = model.predict(dvalid)\n#平均平方二乗誤差でスコアを算出\n    score = np.sqrt(mean_squared_error(va_y, va_pred))\n    score_list.append(score)\n    continue\n        \nprint(np.mean(score_list))\n    \n#test予測(二値の予測値)\ndtrain = xgb.DMatrix(train_x_2, label=train_y)\ndtest = xgb.DMatrix(test_x_2)\n\nmodel = xgb.train(params, dtrain, num_round)\ntest_pred = model.predict(dtest)\n\ndata2 = {\n    'Id'       : test['Id'],\n    'Saleprice': test_pred\n}\ndf2 = pd.DataFrame(data2)\nprint(df2)\n\ndf2.to_csv(\"test6.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T15:05:14.551792Z","iopub.execute_input":"2022-12-25T15:05:14.552301Z","iopub.status.idle":"2022-12-25T15:05:39.027419Z","shell.execute_reply.started":"2022-12-25T15:05:14.552262Z","shell.execute_reply":"2022-12-25T15:05:39.026331Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"25619.333069088043\n        Id      Saleprice\n0     1461  127439.148438\n1     1462  157320.328125\n2     1463  183088.781250\n3     1464  188072.531250\n4     1465  188530.781250\n...    ...            ...\n1454  2915   82648.085938\n1455  2916   86053.242188\n1456  2917  158930.984375\n1457  2918  117507.367188\n1458  2919  219117.078125\n\n[1459 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"#手動でパラメータチューニングする場合\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n#ハイパーパラメータの設定\nfor i in [0.0,0.1,0.2,0.3,0.4]:\n    score_list = []\n    params = {\n        'booster':'gbtree',\n        'objective': 'reg:squarederror',\n        'eta':0.01,\n        'gamma':i,\n        'alpha':0.0,\n        'lambda':1.0,\n        'min_child_weight':2,\n        'max_depth':6,\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'colsample_bylevel':0.4,\n        'random_state': 71\n        }\n    num_round=1000\n\n    train_x_2 = train_x\n    test_x_2 = test_x\n\n#バリデーション\n    for tr_idx, va_idx in kf.split(train_x_2):\n        tr_x, va_x = train_x_2.iloc[tr_idx], train_x_2.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\n        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n        dvalid = xgb.DMatrix(va_x, label=va_y)\n\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\n        watchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\n        model = xgb.train(params, dtrain, num_round)\n\n        va_pred = model.predict(dvalid)\n#平均平方二乗誤差でスコアを算出\n        score = np.sqrt(mean_squared_error(va_y, va_pred))\n        score_list.append(score)\n        continue\n        \n    print(\"i = %s\" %(i))    \n    print(np.mean(score_list))\n    \n#test予測(二値の予測値)\n    dtrain = xgb.DMatrix(train_x_2, label=train_y)\n    dtest = xgb.DMatrix(test_x_2)\n\n    model = xgb.train(params, dtrain, num_round)\n    test_pred = model.predict(dtest)\n\n    data2 = {\n        'Id'       : test['Id'],\n        'Saleprice': test_pred\n    }\n    df2 = pd.DataFrame(data2)\n    print(df2)\n\n    df2.to_csv(\"test%s.csv\" %(i), index = False)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T15:05:47.607105Z","iopub.execute_input":"2022-12-25T15:05:47.607638Z","iopub.status.idle":"2022-12-25T15:07:45.047314Z","shell.execute_reply.started":"2022-12-25T15:05:47.607581Z","shell.execute_reply":"2022-12-25T15:07:45.046170Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"i = 0.0\n25619.333069088043\n        Id      Saleprice\n0     1461  127439.148438\n1     1462  157320.328125\n2     1463  183088.781250\n3     1464  188072.531250\n4     1465  188530.781250\n...    ...            ...\n1454  2915   82648.085938\n1455  2916   86053.242188\n1456  2917  158930.984375\n1457  2918  117507.367188\n1458  2919  219117.078125\n\n[1459 rows x 2 columns]\ni = 0.1\n25619.333069088043\n        Id      Saleprice\n0     1461  127439.148438\n1     1462  157320.328125\n2     1463  183088.781250\n3     1464  188072.531250\n4     1465  188530.781250\n...    ...            ...\n1454  2915   82648.085938\n1455  2916   86053.242188\n1456  2917  158930.984375\n1457  2918  117507.367188\n1458  2919  219117.078125\n\n[1459 rows x 2 columns]\ni = 0.2\n25619.333069088043\n        Id      Saleprice\n0     1461  127439.148438\n1     1462  157320.328125\n2     1463  183088.781250\n3     1464  188072.531250\n4     1465  188530.781250\n...    ...            ...\n1454  2915   82648.085938\n1455  2916   86053.242188\n1456  2917  158930.984375\n1457  2918  117507.367188\n1458  2919  219117.078125\n\n[1459 rows x 2 columns]\ni = 0.3\n25619.333069088043\n        Id      Saleprice\n0     1461  127439.148438\n1     1462  157320.328125\n2     1463  183088.781250\n3     1464  188072.531250\n4     1465  188530.781250\n...    ...            ...\n1454  2915   82648.085938\n1455  2916   86053.242188\n1456  2917  158930.984375\n1457  2918  117507.367188\n1458  2919  219117.078125\n\n[1459 rows x 2 columns]\ni = 0.4\n25619.333069088043\n        Id      Saleprice\n0     1461  127439.148438\n1     1462  157320.328125\n2     1463  183088.781250\n3     1464  188072.531250\n4     1465  188530.781250\n...    ...            ...\n1454  2915   82648.085938\n1455  2916   86053.242188\n1456  2917  158930.984375\n1457  2918  117507.367188\n1458  2919  219117.078125\n\n[1459 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"#グリッドサーチでパラメータチューニング\nimport itertools\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n#ハイパーパラメータの設定\nparam_space = {\n    'max_depth':[2,3,4,5,6,7,8,9,10],\n    'min_child_weight':[1,2,3,4],\n    }\nparam_combinations = itertools.product(param_space['max_depth'],param_space['min_child_weight'])\nparameters = []\nscores = []\nscore_list = []\nnum_round=1000\ntrain_x_2 = train_x\ntest_x_2 = test_x\n\nfor  max_depth, min_child_weight in param_combinations:\n    params = {\n        'booster':'gbtree',\n        'objective': 'reg:squarederror',\n        'eta':0.01,\n        'gamma':0.0,\n        'alpha':0.0,\n        'lambda':1.0,\n        'max_depth':max_depth,\n        'min_child_weight':min_child_weight,\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'colsample_bylevel':0.4,\n        'random_state': 71\n        }\n\n#バリデーション\n    for tr_idx, va_idx in kf.split(train_x_2):\n        tr_x, va_x = train_x_2.iloc[tr_idx], train_x_2.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n#特徴量と目的変数をxgboostのデータ構造に変換する\n        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n        dvalid = xgb.DMatrix(va_x, label=va_y)\n\n#学習の実行\n#バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n#watchlistには学習データおよびバリデーションデータをセットする\n        watchlist = [(dtrain, 'train'), (dvalid, 'eval') ]\n        model = xgb.train(params, dtrain, num_round)\n\n        va_pred = model.predict(dvalid)\n#平均平方二乗誤差でスコアを算出\n        score = np.sqrt(mean_squared_error(va_y, va_pred))\n        score_list.append(score)\n        continue\n    print(f'max_depth: %s, min_chind_weight: %s'%(max_depth, min_child_weight))\n    print(np.mean(score_list))\n    \n    parameters.append((max_depth,min_child_weight))\n    scores.append(np.mean(score_list))\n\nbest_idx = np.argsort(scores)[0]\nbest_param = parameters[best_idx]\nprint(f'max_depth: {best_param[0]}, min_chind_weight: {best_param[1]}')","metadata":{"execution":{"iopub.status.busy":"2022-12-25T15:07:45.049709Z","iopub.execute_input":"2022-12-25T15:07:45.050515Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"max_depth: 2, min_chind_weight: 1\n28482.208006717312\nmax_depth: 2, min_chind_weight: 2\n28728.275265094017\nmax_depth: 2, min_chind_weight: 3\n28725.64919202962\nmax_depth: 2, min_chind_weight: 4\n28825.05988001889\nmax_depth: 3, min_chind_weight: 1\n28400.56497763006\nmax_depth: 3, min_chind_weight: 2\n28131.04544673154\nmax_depth: 3, min_chind_weight: 3\n27949.976328931574\nmax_depth: 3, min_chind_weight: 4\n27863.42323178389\nmax_depth: 4, min_chind_weight: 1\n27633.332800352495\nmax_depth: 4, min_chind_weight: 2\n27442.261478368746\nmax_depth: 4, min_chind_weight: 3\n27326.20903591865\nmax_depth: 4, min_chind_weight: 4\n27246.572957171797\nmax_depth: 5, min_chind_weight: 1\n27137.108771866348\nmax_depth: 5, min_chind_weight: 2\n27021.492709141625\nmax_depth: 5, min_chind_weight: 3\n26947.164351111973\nmax_depth: 5, min_chind_weight: 4\n26900.975234852296\nmax_depth: 6, min_chind_weight: 1\n26831.683195925852\nmax_depth: 6, min_chind_weight: 2\n26764.33041110153\nmax_depth: 6, min_chind_weight: 3\n26719.327553150764\nmax_depth: 6, min_chind_weight: 4\n26689.8342073065\nmax_depth: 7, min_chind_weight: 1\n26647.77945075548\n","output_type":"stream"}]}]}